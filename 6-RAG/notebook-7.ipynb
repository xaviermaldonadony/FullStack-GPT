{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed777a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building with glass doors that let in gritty dust. The hallway smells of boiled cabbage and old rag mats. Inside, there is a poster of a man\\'s face with a caption that reads \"BIG BROTHER IS WATCHING YOU.\" The flat is seven flights up, and the building is part of the economy drive in preparation for Hate Week.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate \n",
    "from langchain.schema.runnable import RunnablePassthrough \n",
    "\n",
    "# 6.7\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"../.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\\x18\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "# load files\n",
    "loader = UnstructuredFileLoader(\"../files/chapter-1.text\")\n",
    "\n",
    "# split files, into small docs makes it easier for llm to read \n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings are vector representation, the meaning behind the text docsk\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "# takes docs and embeddings. we use the store to later make seraches and get related data/info\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer\"\n",
    "    \"just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain ={\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "# String given to retirever, retriever returns a list of documents, the string/question also goes as the question in prompt\n",
    "chain.invoke(\"Describe Victory Mansions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
